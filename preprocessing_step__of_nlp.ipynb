{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mNAwXNa-Bui9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ecf03e-7eb2-40be-ab1f-fff3b83eafb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Performing first process of the nlp procedures Coverts into tokens in the form of words, corpus or a documents using nltk library\n",
        "##  Tokenization\n",
        "## Sentence-->paragraphs\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "text = \"Natural language processing's a type of machine learning in which computers learn from data.To do that, the computer is trained on a large dataset and then makes predictions or decisions based on that training.Then, when presented with unstructured data, the program can apply its training to understand text, find information, or generate human language.For example, a natural language algorithm trained on a dataset of handwritten words and sentences might learn to read and classify handwritten texts.After training, the algorithm can then be used to classify new, unseen images of handwriting based on the patterns it learned.\"\n",
        "paragraph = text.lower()\n",
        "print(paragraph)\n",
        "# next process of removing the removing stop words\n",
        "\n",
        "# Get the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "fcNkmDLiB0t0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b41609-acf9-4742-d925-acd3936ac2a2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natural language processing's a type of machine learning in which computers learn from data.to do that, the computer is trained on a large dataset and then makes predictions or decisions based on that training.then, when presented with unstructured data, the program can apply its training to understand text, find information, or generate human language.for example, a natural language algorithm trained on a dataset of handwritten words and sentences might learn to read and classify handwritten texts.after training, the algorithm can then be used to classify new, unseen images of handwriting based on the patterns it learned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize, TreebankWordTokenizer\n",
        "\n",
        "# 1. Sentence Tokenization\n",
        "# Split text into individual sentences using punctuation and language rules\n",
        "# Handles abbreviations, decimal points, and various sentence endings correctly\n",
        "tokens = sent_tokenize(paragraph)\n",
        "print(tokens)\n",
        "\n",
        "# 2. Word Tokenization\n",
        "# Break text into individual words and punctuation\n",
        "# Preserves contractions and handles special cases like abbreviations\n",
        "words = word_tokenize(paragraph)\n",
        "print(words)\n",
        "\n",
        "# 3. Word Punctuation Tokenization\n",
        "# Splits on both word boundaries and punctuation marks\n",
        "# Results in more granular tokenization than word_tokenize\n",
        "# Useful when you need punctuation as separate tokens\n",
        "wordp = wordpunct_tokenize(paragraph)\n",
        "print(wordp)\n",
        "\n",
        "# 4. Treebank Word Tokenization\n",
        "# Implements the Penn Treebank tokenization rules\n",
        "# More sophisticated than wordpunct_tokenize for English text\n",
        "# Handles contractions, possessives, and hyphenated words better\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "snowball = tokenizer.tokenize(paragraph)\n",
        "\n",
        "filtered_text = [word for word in snowball if word not in stop_words]\n",
        "filtertext = \" \".join(filtered_text)\n",
        "print(filtertext)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUik7KcREFt-",
        "outputId": "f0becd3e-b11e-4547-d3fb-ddf6ed35022e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"natural language processing's a type of machine learning in which computers learn from data.to do that, the computer is trained on a large dataset and then makes predictions or decisions based on that training.then, when presented with unstructured data, the program can apply its training to understand text, find information, or generate human language.for example, a natural language algorithm trained on a dataset of handwritten words and sentences might learn to read and classify handwritten texts.after training, the algorithm can then be used to classify new, unseen images of handwriting based on the patterns it learned.\"]\n",
            "['natural', 'language', 'processing', \"'s\", 'a', 'type', 'of', 'machine', 'learning', 'in', 'which', 'computers', 'learn', 'from', 'data.to', 'do', 'that', ',', 'the', 'computer', 'is', 'trained', 'on', 'a', 'large', 'dataset', 'and', 'then', 'makes', 'predictions', 'or', 'decisions', 'based', 'on', 'that', 'training.then', ',', 'when', 'presented', 'with', 'unstructured', 'data', ',', 'the', 'program', 'can', 'apply', 'its', 'training', 'to', 'understand', 'text', ',', 'find', 'information', ',', 'or', 'generate', 'human', 'language.for', 'example', ',', 'a', 'natural', 'language', 'algorithm', 'trained', 'on', 'a', 'dataset', 'of', 'handwritten', 'words', 'and', 'sentences', 'might', 'learn', 'to', 'read', 'and', 'classify', 'handwritten', 'texts.after', 'training', ',', 'the', 'algorithm', 'can', 'then', 'be', 'used', 'to', 'classify', 'new', ',', 'unseen', 'images', 'of', 'handwriting', 'based', 'on', 'the', 'patterns', 'it', 'learned', '.']\n",
            "['natural', 'language', 'processing', \"'\", 's', 'a', 'type', 'of', 'machine', 'learning', 'in', 'which', 'computers', 'learn', 'from', 'data', '.', 'to', 'do', 'that', ',', 'the', 'computer', 'is', 'trained', 'on', 'a', 'large', 'dataset', 'and', 'then', 'makes', 'predictions', 'or', 'decisions', 'based', 'on', 'that', 'training', '.', 'then', ',', 'when', 'presented', 'with', 'unstructured', 'data', ',', 'the', 'program', 'can', 'apply', 'its', 'training', 'to', 'understand', 'text', ',', 'find', 'information', ',', 'or', 'generate', 'human', 'language', '.', 'for', 'example', ',', 'a', 'natural', 'language', 'algorithm', 'trained', 'on', 'a', 'dataset', 'of', 'handwritten', 'words', 'and', 'sentences', 'might', 'learn', 'to', 'read', 'and', 'classify', 'handwritten', 'texts', '.', 'after', 'training', ',', 'the', 'algorithm', 'can', 'then', 'be', 'used', 'to', 'classify', 'new', ',', 'unseen', 'images', 'of', 'handwriting', 'based', 'on', 'the', 'patterns', 'it', 'learned', '.']\n",
            "natural language processing 's type machine learning computers learn data.to , computer trained large dataset makes predictions decisions based training.then , presented unstructured data , program apply training understand text , find information , generate human language.for example , natural language algorithm trained dataset handwritten words sentences might learn read classify handwritten texts.after training , algorithm used classify new , unseen images handwriting based patterns learned .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bp8F7htViubg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming And Its Types\n",
        "#Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).\n",
        "# Classification Problem\n",
        "# Comments of product is a positive review or negative review\n",
        "# Reviews----> eating, eat,eaten [going,gone,goes]--->go\n",
        "\n",
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n",
        "\n",
        "from nltk.stem import PorterStemmer, RegexpStemmer, SnowballStemmer\n",
        "# 1. Porter Stemmer\n",
        "# Most widely used stemmer\n",
        "# Uses a set of rules to strip suffixes from English words\n",
        "porter = PorterStemmer()\n",
        "print(\"Porter Stemmer Results:\")\n",
        "print(\"-\" * 40)\n",
        "for word in filtered_text:\n",
        "    stemmed = porter.stem(word)\n",
        "    print(f\"{word:15} → {stemmed}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# 2. Regexp Stemmer\n",
        "# Simple stemmer that strips any suffix matching the regular expression\n",
        "# In this case, 'v' will remove 'v' from the end of words if present\n",
        "regex = RegexpStemmer('v')\n",
        "print(\"Regex Stemmer Results:\")\n",
        "print(\"-\" * 40)\n",
        "for word in words:\n",
        "    stemmed = regex.stem(word)\n",
        "    print(f\"{word:15} → {stemmed}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# 3. Snowball Stemmer\n",
        "# More aggressive than Porter Stemmer\n",
        "# Also known as Porter2 stemmer\n",
        "# Supports multiple languages (using 'english' here)\n",
        "snowball = SnowballStemmer('english')\n",
        "print(\"Snowball Stemmer Results:\")\n",
        "print(\"-\" * 40)\n",
        "for word in words:\n",
        "    stemmed = snowball.stem(word)\n",
        "    print(f\"{word:15} → {stemmed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYyaPqrtF7z1",
        "outputId": "96c170d1-ec74-4ad9-8d66-530682d2f346"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer Results:\n",
            "----------------------------------------\n",
            "natural         → natur\n",
            "language        → languag\n",
            "processing      → process\n",
            "'s              → 's\n",
            "type            → type\n",
            "machine         → machin\n",
            "learning        → learn\n",
            "computers       → comput\n",
            "learn           → learn\n",
            "data.to         → data.to\n",
            ",               → ,\n",
            "computer        → comput\n",
            "trained         → train\n",
            "large           → larg\n",
            "dataset         → dataset\n",
            "makes           → make\n",
            "predictions     → predict\n",
            "decisions       → decis\n",
            "based           → base\n",
            "training.then   → training.then\n",
            ",               → ,\n",
            "presented       → present\n",
            "unstructured    → unstructur\n",
            "data            → data\n",
            ",               → ,\n",
            "program         → program\n",
            "apply           → appli\n",
            "training        → train\n",
            "understand      → understand\n",
            "text            → text\n",
            ",               → ,\n",
            "find            → find\n",
            "information     → inform\n",
            ",               → ,\n",
            "generate        → gener\n",
            "human           → human\n",
            "language.for    → language.for\n",
            "example         → exampl\n",
            ",               → ,\n",
            "natural         → natur\n",
            "language        → languag\n",
            "algorithm       → algorithm\n",
            "trained         → train\n",
            "dataset         → dataset\n",
            "handwritten     → handwritten\n",
            "words           → word\n",
            "sentences       → sentenc\n",
            "might           → might\n",
            "learn           → learn\n",
            "read            → read\n",
            "classify        → classifi\n",
            "handwritten     → handwritten\n",
            "texts.after     → texts.aft\n",
            "training        → train\n",
            ",               → ,\n",
            "algorithm       → algorithm\n",
            "used            → use\n",
            "classify        → classifi\n",
            "new             → new\n",
            ",               → ,\n",
            "unseen          → unseen\n",
            "images          → imag\n",
            "handwriting     → handwrit\n",
            "based           → base\n",
            "patterns        → pattern\n",
            "learned         → learn\n",
            ".               → .\n",
            "\n",
            "\n",
            "Regex Stemmer Results:\n",
            "----------------------------------------\n",
            "eating          → eating\n",
            "eats            → eats\n",
            "eaten           → eaten\n",
            "writing         → writing\n",
            "writes          → writes\n",
            "programming     → programming\n",
            "programs        → programs\n",
            "history         → history\n",
            "finally         → finally\n",
            "finalized       → finalized\n",
            "\n",
            "\n",
            "Snowball Stemmer Results:\n",
            "----------------------------------------\n",
            "eating          → eat\n",
            "eats            → eat\n",
            "eaten           → eaten\n",
            "writing         → write\n",
            "writes          → write\n",
            "programming     → program\n",
            "programs        → program\n",
            "history         → histori\n",
            "finally         → final\n",
            "finalized       → final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data (if not already downloaded)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text\n",
        "text = \"\"\"Natural language processing's a type of machine learning in which computers learn from data.\n",
        "To do that, the computer is trained on a large dataset and then makes predictions or decisions based on that training.\n",
        "Then, when presented with unstructured data, the program can apply its training to understand text, find information,\n",
        "or generate human language. For example, a natural language algorithm trained on a dataset of handwritten words and\n",
        "sentences might learn to read and classify handwritten texts. After training, the algorithm can then be used to\n",
        "classify new, unseen images of handwriting based on the patterns it learned.\"\"\"\n",
        "\n",
        "# Convert to lowercase\n",
        "paragraph = text.lower()\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize the text\n",
        "word_tokens = word_tokenize(paragraph)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "\n",
        "# Initialize stemmers and lemmatizer\n",
        "porter = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Print comparison of stemming and lemmatization\n",
        "print(\"Comparison of Stemming and Lemmatization:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Original Word':20} {'Porter Stem':20} {'Lemmatization':20}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for word in filtered_text:\n",
        "    # Get POS tag for better lemmatization\n",
        "    pos_tag = nltk.pos_tag([word])[0][1]\n",
        "\n",
        "    # Convert POS tag to WordNet POS tag\n",
        "    wordnet_pos = 'n'  # default is noun\n",
        "    if pos_tag.startswith('V'):\n",
        "        wordnet_pos = 'v'  # verb\n",
        "    elif pos_tag.startswith('J'):\n",
        "        wordnet_pos = 'a'  # adjective\n",
        "    elif pos_tag.startswith('R'):\n",
        "        wordnet_pos = 'r'  # adverb\n",
        "\n",
        "    stemmed = porter.stem(word)\n",
        "    lemmatized = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
        "    print(f\"{word:20} {stemmed:20} {lemmatized:20}\")\n",
        "\n",
        "# Create processed versions\n",
        "stemmed_text = [porter.stem(word) for word in filtered_text]\n",
        "lemmatized_text = [lemmatizer.lemmatize(word, pos=wordnet_pos) for word in filtered_text]\n",
        "\n",
        "print(\"\\nProcessed Text Versions:\")\n",
        "print(\"\\n1. After Stemming:\")\n",
        "print(' '.join(stemmed_text))\n",
        "print(\"\\n2. After Lemmatization:\")\n",
        "print(' '.join(lemmatized_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS4jtSu_CF1X",
        "outputId": "ff3cec1e-faba-4328-8ded-53710ac3c3f1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of Stemming and Lemmatization:\n",
            "------------------------------------------------------------\n",
            "Original Word        Porter Stem          Lemmatization       \n",
            "------------------------------------------------------------\n",
            "natural              natur                natural             \n",
            "language             languag              language            \n",
            "processing           process              processing          \n",
            "'s                   's                   's                  \n",
            "type                 type                 type                \n",
            "machine              machin               machine             \n",
            "learning             learn                learn               \n",
            "computers            comput               computer            \n",
            "learn                learn                learn               \n",
            "data                 data                 data                \n",
            ".                    .                    .                   \n",
            ",                    ,                    ,                   \n",
            "computer             comput               computer            \n",
            "trained              train                train               \n",
            "large                larg                 large               \n",
            "dataset              dataset              dataset             \n",
            "makes                make                 make                \n",
            "predictions          predict              prediction          \n",
            "decisions            decis                decision            \n",
            "based                base                 base                \n",
            "training             train                training            \n",
            ".                    .                    .                   \n",
            ",                    ,                    ,                   \n",
            "presented            present              present             \n",
            "unstructured         unstructur           unstructured        \n",
            "data                 data                 data                \n",
            ",                    ,                    ,                   \n",
            "program              program              program             \n",
            "apply                appli                apply               \n",
            "training             train                training            \n",
            "understand           understand           understand          \n",
            "text                 text                 text                \n",
            ",                    ,                    ,                   \n",
            "find                 find                 find                \n",
            "information          inform               information         \n",
            ",                    ,                    ,                   \n",
            "generate             gener                generate            \n",
            "human                human                human               \n",
            "language             languag              language            \n",
            ".                    .                    .                   \n",
            "example              exampl               example             \n",
            ",                    ,                    ,                   \n",
            "natural              natur                natural             \n",
            "language             languag              language            \n",
            "algorithm            algorithm            algorithm           \n",
            "trained              train                train               \n",
            "dataset              dataset              dataset             \n",
            "handwritten          handwritten          handwritten         \n",
            "words                word                 word                \n",
            "sentences            sentenc              sentence            \n",
            "might                might                might               \n",
            "learn                learn                learn               \n",
            "read                 read                 read                \n",
            "classify             classifi             classify            \n",
            "handwritten          handwritten          handwritten         \n",
            "texts                text                 text                \n",
            ".                    .                    .                   \n",
            "training             train                training            \n",
            ",                    ,                    ,                   \n",
            "algorithm            algorithm            algorithm           \n",
            "used                 use                  use                 \n",
            "classify             classifi             classify            \n",
            "new                  new                  new                 \n",
            ",                    ,                    ,                   \n",
            "unseen               unseen               unseen              \n",
            "images               imag                 image               \n",
            "handwriting          handwrit             handwrite           \n",
            "based                base                 base                \n",
            "patterns             pattern              pattern             \n",
            "learned              learn                learn               \n",
            ".                    .                    .                   \n",
            "\n",
            "Processed Text Versions:\n",
            "\n",
            "1. After Stemming:\n",
            "natur languag process 's type machin learn comput learn data . , comput train larg dataset make predict decis base train . , present unstructur data , program appli train understand text , find inform , gener human languag . exampl , natur languag algorithm train dataset handwritten word sentenc might learn read classifi handwritten text . train , algorithm use classifi new , unseen imag handwrit base pattern learn .\n",
            "\n",
            "2. After Lemmatization:\n",
            "natural language processing 's type machine learning computer learn data . , computer trained large dataset make prediction decision based training . , presented unstructured data , program apply training understand text , find information , generate human language . example , natural language algorithm trained dataset handwritten word sentence might learn read classify handwritten text . training , algorithm used classify new , unseen image handwriting based pattern learned .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lVn7NWm0EEva"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBUh1f-XDbGw"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aajRKuSLGtop"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u5XqI0nJG97Y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_mT7uUj7Hkik"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2KanPQHUO5LY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}